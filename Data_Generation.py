from langchain.chains import RetrievalQA
from langchain_ollama import OllamaLLM as Ollama  # ‚úÖ Mise √† jour
from Data_Indexing_Storage import load_faiss_index


# ‚õìÔ∏è Cr√©ation de la cha√Æne RAG avec FAISS et Ollama
def get_rag_chain():
    vectorstore = load_faiss_index("faiss_index")
    retriever = vectorstore.as_retriever()

    llm = Ollama(model="llama3.2:1b")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True
    )

    return qa_chain, llm  # ‚úÖ On retourne deux objets



# üí¨ G√©n√©ration d'une r√©ponse enrichie √† partir du contexte + question
def generate_answer_with_context(llm, context, question):

    full_prompt = f"""
Tu es un assistant sp√©cialis√© en optimisation des achats. En te basant sur les donn√©es ci-dessous, r√©ponds clairement √† la question de l'utilisateur.

Donn√©es :
{context}

Question :
{question}

üìù Inclue dans ta r√©ponse si possible :
- Score global
- Taux de conformit√©
- Taux de respect des d√©lais
- Cat√©gorie (fiable, moyen, risqu√©)
- Co√ªt unitaire
"""
    response = llm.invoke(full_prompt)
    return response


if __name__ == "__main__":
    # ‚úÖ Correction ici : on s√©pare les deux objets
    qa_chain, llm = get_rag_chain()

    user_question = "Quel est le fournisseur le plus fiable pour les articles de type Mat√©riel ?"
    result = qa_chain.invoke({"query": user_question})

    # ‚úÖ On extrait les documents sources
    context = "\n\n".join([doc.page_content for doc in result['source_documents']])

    # ‚úÖ R√©utilisation du LLM pour le prompt enrichi
    final_answer = generate_answer_with_context(llm, context, user_question)

    print("üí¨ R√©ponse enrichie :\n", final_answer)

